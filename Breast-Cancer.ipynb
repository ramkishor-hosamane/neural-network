{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Breast-Cancer </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0    ...        25.380          17.33           184.60      2019.0   \n",
       "1    ...        24.990          23.41           158.80      1956.0   \n",
       "2    ...        23.570          25.53           152.50      1709.0   \n",
       "3    ...        14.910          26.50            98.87       567.7   \n",
       "4    ...        22.540          16.67           152.20      1575.0   \n",
       "..   ...           ...            ...              ...         ...   \n",
       "564  ...        25.450          26.40           166.10      2027.0   \n",
       "565  ...        23.690          38.25           155.00      1731.0   \n",
       "566  ...        18.980          34.12           126.70      1124.0   \n",
       "567  ...        25.740          39.42           184.60      1821.0   \n",
       "568  ...         9.456          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"breast_cancer.csv\")\n",
    "df.drop(\"Unnamed: 32\",inplace=True,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      M\n",
       "1      M\n",
       "2      M\n",
       "3      M\n",
       "4      M\n",
       "      ..\n",
       "564    M\n",
       "565    M\n",
       "566    M\n",
       "567    M\n",
       "568    B\n",
       "Name: diagnosis, Length: 569, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,2:]\n",
    "Y = df.iloc[:,1]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y= to_categorical(Y,num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(30,)))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(rate=0.1))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                992       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 3,290\n",
      "Trainable params: 3,230\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "512/512 [==============================] - 0s 847us/step - loss: 0.5882 - accuracy: 0.7207\n",
      "Epoch 2/100\n",
      "512/512 [==============================] - 0s 74us/step - loss: 0.3617 - accuracy: 0.8926\n",
      "Epoch 3/100\n",
      "512/512 [==============================] - 0s 77us/step - loss: 0.2229 - accuracy: 0.9336\n",
      "Epoch 4/100\n",
      "512/512 [==============================] - 0s 82us/step - loss: 0.1627 - accuracy: 0.9473\n",
      "Epoch 5/100\n",
      "512/512 [==============================] - 0s 83us/step - loss: 0.1232 - accuracy: 0.9531\n",
      "Epoch 6/100\n",
      "512/512 [==============================] - 0s 82us/step - loss: 0.1334 - accuracy: 0.9551\n",
      "Epoch 7/100\n",
      "512/512 [==============================] - 0s 79us/step - loss: 0.0841 - accuracy: 0.9766\n",
      "Epoch 8/100\n",
      "512/512 [==============================] - 0s 80us/step - loss: 0.1087 - accuracy: 0.9609\n",
      "Epoch 9/100\n",
      "512/512 [==============================] - 0s 84us/step - loss: 0.0904 - accuracy: 0.9688\n",
      "Epoch 10/100\n",
      "512/512 [==============================] - 0s 82us/step - loss: 0.0949 - accuracy: 0.9609\n",
      "Epoch 11/100\n",
      "512/512 [==============================] - 0s 83us/step - loss: 0.0816 - accuracy: 0.9727\n",
      "Epoch 12/100\n",
      "512/512 [==============================] - 0s 72us/step - loss: 0.0654 - accuracy: 0.9766\n",
      "Epoch 13/100\n",
      "512/512 [==============================] - 0s 80us/step - loss: 0.0961 - accuracy: 0.9609\n",
      "Epoch 14/100\n",
      "512/512 [==============================] - 0s 73us/step - loss: 0.0783 - accuracy: 0.9707\n",
      "Epoch 15/100\n",
      "512/512 [==============================] - 0s 80us/step - loss: 0.0972 - accuracy: 0.9668\n",
      "Epoch 16/100\n",
      "512/512 [==============================] - 0s 80us/step - loss: 0.1130 - accuracy: 0.9551\n",
      "Epoch 17/100\n",
      "512/512 [==============================] - 0s 86us/step - loss: 0.1025 - accuracy: 0.9629\n",
      "Epoch 18/100\n",
      "512/512 [==============================] - 0s 84us/step - loss: 0.0891 - accuracy: 0.9688\n",
      "Epoch 19/100\n",
      "512/512 [==============================] - 0s 76us/step - loss: 0.0694 - accuracy: 0.9766\n",
      "Epoch 20/100\n",
      "512/512 [==============================] - 0s 82us/step - loss: 0.0773 - accuracy: 0.9648\n",
      "Epoch 21/100\n",
      "512/512 [==============================] - 0s 81us/step - loss: 0.0876 - accuracy: 0.9707\n",
      "Epoch 22/100\n",
      "512/512 [==============================] - 0s 77us/step - loss: 0.0671 - accuracy: 0.9746\n",
      "Epoch 23/100\n",
      "512/512 [==============================] - 0s 88us/step - loss: 0.0604 - accuracy: 0.9766\n",
      "Epoch 24/100\n",
      "512/512 [==============================] - 0s 77us/step - loss: 0.0466 - accuracy: 0.9824\n",
      "Epoch 25/100\n",
      "512/512 [==============================] - 0s 85us/step - loss: 0.0829 - accuracy: 0.9648\n",
      "Epoch 26/100\n",
      "512/512 [==============================] - 0s 78us/step - loss: 0.0712 - accuracy: 0.9688\n",
      "Epoch 27/100\n",
      "512/512 [==============================] - 0s 79us/step - loss: 0.0475 - accuracy: 0.9844\n",
      "Epoch 28/100\n",
      "512/512 [==============================] - 0s 78us/step - loss: 0.0538 - accuracy: 0.9766\n",
      "Epoch 29/100\n",
      "512/512 [==============================] - 0s 81us/step - loss: 0.0887 - accuracy: 0.9688\n",
      "Epoch 30/100\n",
      "512/512 [==============================] - 0s 79us/step - loss: 0.0731 - accuracy: 0.9688\n",
      "Epoch 31/100\n",
      "512/512 [==============================] - 0s 90us/step - loss: 0.0563 - accuracy: 0.9863\n",
      "Epoch 32/100\n",
      "512/512 [==============================] - 0s 85us/step - loss: 0.0449 - accuracy: 0.9844\n",
      "Epoch 33/100\n",
      "512/512 [==============================] - 0s 86us/step - loss: 0.0426 - accuracy: 0.9844\n",
      "Epoch 34/100\n",
      "512/512 [==============================] - 0s 79us/step - loss: 0.0454 - accuracy: 0.9863\n",
      "Epoch 35/100\n",
      "512/512 [==============================] - 0s 81us/step - loss: 0.0742 - accuracy: 0.9688\n",
      "Epoch 36/100\n",
      "512/512 [==============================] - 0s 82us/step - loss: 0.0321 - accuracy: 0.9863\n",
      "Epoch 37/100\n",
      "512/512 [==============================] - 0s 82us/step - loss: 0.0754 - accuracy: 0.9766\n",
      "Epoch 38/100\n",
      "512/512 [==============================] - 0s 81us/step - loss: 0.0776 - accuracy: 0.9688\n",
      "Epoch 39/100\n",
      "512/512 [==============================] - 0s 91us/step - loss: 0.0648 - accuracy: 0.9766\n",
      "Epoch 40/100\n",
      "512/512 [==============================] - 0s 78us/step - loss: 0.0768 - accuracy: 0.9727\n",
      "Epoch 41/100\n",
      "512/512 [==============================] - 0s 80us/step - loss: 0.0307 - accuracy: 0.9883\n",
      "Epoch 42/100\n",
      "512/512 [==============================] - 0s 86us/step - loss: 0.0463 - accuracy: 0.9844\n",
      "Epoch 43/100\n",
      "512/512 [==============================] - 0s 92us/step - loss: 0.0510 - accuracy: 0.9805\n",
      "Epoch 44/100\n",
      "512/512 [==============================] - 0s 85us/step - loss: 0.0603 - accuracy: 0.9766\n",
      "Epoch 45/100\n",
      "512/512 [==============================] - 0s 79us/step - loss: 0.1099 - accuracy: 0.9668\n",
      "Epoch 46/100\n",
      "512/512 [==============================] - 0s 89us/step - loss: 0.0436 - accuracy: 0.9824\n",
      "Epoch 47/100\n",
      "512/512 [==============================] - 0s 91us/step - loss: 0.0814 - accuracy: 0.9648\n",
      "Epoch 48/100\n",
      "512/512 [==============================] - 0s 92us/step - loss: 0.0415 - accuracy: 0.9844\n",
      "Epoch 49/100\n",
      "512/512 [==============================] - 0s 96us/step - loss: 0.0383 - accuracy: 0.9844\n",
      "Epoch 50/100\n",
      "512/512 [==============================] - 0s 77us/step - loss: 0.0440 - accuracy: 0.9883\n",
      "Epoch 51/100\n",
      "512/512 [==============================] - 0s 76us/step - loss: 0.0333 - accuracy: 0.9863\n",
      "Epoch 52/100\n",
      "512/512 [==============================] - 0s 74us/step - loss: 0.0532 - accuracy: 0.9805\n",
      "Epoch 53/100\n",
      "512/512 [==============================] - 0s 70us/step - loss: 0.0416 - accuracy: 0.9863\n",
      "Epoch 54/100\n",
      "512/512 [==============================] - 0s 71us/step - loss: 0.0422 - accuracy: 0.9844\n",
      "Epoch 55/100\n",
      "512/512 [==============================] - 0s 75us/step - loss: 0.0700 - accuracy: 0.9766\n",
      "Epoch 56/100\n",
      "512/512 [==============================] - 0s 72us/step - loss: 0.0288 - accuracy: 0.9902\n",
      "Epoch 57/100\n",
      "512/512 [==============================] - 0s 74us/step - loss: 0.0255 - accuracy: 0.9922\n",
      "Epoch 58/100\n",
      "512/512 [==============================] - 0s 76us/step - loss: 0.0432 - accuracy: 0.9883\n",
      "Epoch 59/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0316 - accuracy: 0.9863\n",
      "Epoch 60/100\n",
      "512/512 [==============================] - 0s 74us/step - loss: 0.0271 - accuracy: 0.9922\n",
      "Epoch 61/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0832 - accuracy: 0.9668\n",
      "Epoch 62/100\n",
      "512/512 [==============================] - 0s 73us/step - loss: 0.0448 - accuracy: 0.9824\n",
      "Epoch 63/100\n",
      "512/512 [==============================] - 0s 82us/step - loss: 0.0204 - accuracy: 0.9941\n",
      "Epoch 64/100\n",
      "512/512 [==============================] - 0s 74us/step - loss: 0.0525 - accuracy: 0.9844\n",
      "Epoch 65/100\n",
      "512/512 [==============================] - 0s 71us/step - loss: 0.0415 - accuracy: 0.9902\n",
      "Epoch 66/100\n",
      "512/512 [==============================] - 0s 71us/step - loss: 0.0194 - accuracy: 0.9961\n",
      "Epoch 67/100\n",
      "512/512 [==============================] - 0s 79us/step - loss: 0.0535 - accuracy: 0.9824\n",
      "Epoch 68/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0457 - accuracy: 0.9785\n",
      "Epoch 69/100\n",
      "512/512 [==============================] - 0s 71us/step - loss: 0.0340 - accuracy: 0.9863\n",
      "Epoch 70/100\n",
      "512/512 [==============================] - 0s 71us/step - loss: 0.0343 - accuracy: 0.9883\n",
      "Epoch 71/100\n",
      "512/512 [==============================] - 0s 72us/step - loss: 0.0350 - accuracy: 0.9844\n",
      "Epoch 72/100\n",
      "512/512 [==============================] - 0s 76us/step - loss: 0.0555 - accuracy: 0.9824\n",
      "Epoch 73/100\n",
      "512/512 [==============================] - 0s 76us/step - loss: 0.0263 - accuracy: 0.9883\n",
      "Epoch 74/100\n",
      "512/512 [==============================] - 0s 79us/step - loss: 0.0314 - accuracy: 0.9883\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 69us/step - loss: 0.0318 - accuracy: 0.9863\n",
      "Epoch 76/100\n",
      "512/512 [==============================] - 0s 70us/step - loss: 0.0529 - accuracy: 0.9805\n",
      "Epoch 77/100\n",
      "512/512 [==============================] - 0s 70us/step - loss: 0.0403 - accuracy: 0.9844\n",
      "Epoch 78/100\n",
      "512/512 [==============================] - 0s 71us/step - loss: 0.0345 - accuracy: 0.9785\n",
      "Epoch 79/100\n",
      "512/512 [==============================] - 0s 71us/step - loss: 0.0522 - accuracy: 0.9805\n",
      "Epoch 80/100\n",
      "512/512 [==============================] - 0s 72us/step - loss: 0.0574 - accuracy: 0.9727\n",
      "Epoch 81/100\n",
      "512/512 [==============================] - 0s 68us/step - loss: 0.0236 - accuracy: 0.9922\n",
      "Epoch 82/100\n",
      "512/512 [==============================] - 0s 68us/step - loss: 0.0330 - accuracy: 0.9883\n",
      "Epoch 83/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0190 - accuracy: 0.9961\n",
      "Epoch 84/100\n",
      "512/512 [==============================] - 0s 68us/step - loss: 0.0175 - accuracy: 0.9961\n",
      "Epoch 85/100\n",
      "512/512 [==============================] - 0s 70us/step - loss: 0.0249 - accuracy: 0.9941\n",
      "Epoch 86/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0216 - accuracy: 0.9941\n",
      "Epoch 87/100\n",
      "512/512 [==============================] - 0s 71us/step - loss: 0.0173 - accuracy: 0.9902\n",
      "Epoch 88/100\n",
      "512/512 [==============================] - 0s 68us/step - loss: 0.0112 - accuracy: 0.9980\n",
      "Epoch 89/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0157 - accuracy: 0.9941\n",
      "Epoch 90/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0183 - accuracy: 0.9941\n",
      "Epoch 91/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0402 - accuracy: 0.9863\n",
      "Epoch 92/100\n",
      "512/512 [==============================] - 0s 74us/step - loss: 0.0267 - accuracy: 0.9824\n",
      "Epoch 93/100\n",
      "512/512 [==============================] - 0s 67us/step - loss: 0.0334 - accuracy: 0.9883\n",
      "Epoch 94/100\n",
      "512/512 [==============================] - 0s 70us/step - loss: 0.0117 - accuracy: 0.9961\n",
      "Epoch 95/100\n",
      "512/512 [==============================] - 0s 68us/step - loss: 0.0392 - accuracy: 0.9824\n",
      "Epoch 96/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0513 - accuracy: 0.9824\n",
      "Epoch 97/100\n",
      "512/512 [==============================] - 0s 70us/step - loss: 0.0220 - accuracy: 0.9961\n",
      "Epoch 98/100\n",
      "512/512 [==============================] - 0s 76us/step - loss: 0.0453 - accuracy: 0.9844\n",
      "Epoch 99/100\n",
      "512/512 [==============================] - 0s 69us/step - loss: 0.0388 - accuracy: 0.9805\n",
      "Epoch 100/100\n",
      "512/512 [==============================] - 0s 73us/step - loss: 0.0292 - accuracy: 0.9863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f090b053e48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "y_test = y_test.argmax(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 96.49122807017544%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy is {}%\".format(((cm[0][0] + cm[1][1])/57)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.savefig('h.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
